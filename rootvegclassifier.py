# -*- coding: utf-8 -*-
"""RootVegClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PZAK-BcrJCKvd1_BuWeJLuOhicvY3APB
"""

!pip install tensorflow-gpu



import tensorflow as tf
print(tf.__version__)

"""Other key libraries and modules we need to import are numpy for calculation & and keras for the Neural Network model. The Squential model type is imported along with the Dense & Sequential models from keras.layers. to_categorical is also imported from utilities to convert test data sets into catagorical labels.

# New Section
"""

# Imports
import tensorflow
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.utils import to_categorical
from tensorflow import keras

#function for image batching
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#file structure navigation
import os
import numpy as np
import matplotlib as plt
import matplotlib.pyplot as plt
import time

# Create images with white backgrounds
import plotly.io as pio
pio.templates.default = 'none'

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import  GoogleCredentials

# Gain authorization for Google drive
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Locate dataset
fid = drive.ListFile({'q':"title='DataSet.zip'"}).GetList()[0]['id']
f = drive.CreateFile({'id':fid})
f.GetContentFile('DataSet.zip')

f.keys()

!unzip DataSet.zip

"""# Extracting Data From File"""

PATH = '/content/DataSet'

os.listdir(PATH)

"""## Creating datasets"""

train_dir = os.path.join(PATH,'Train')
test_dir = os.path.join(PATH, 'Test')
validate_dir = os.path.join(PATH,'Validate')

train_carrot_dir = os.path.join(train_dir, 'Carrot')
train_parsnip_dir = os.path.join(train_dir, 'Parsnip')

test_carrot_dir = os.path.join(test_dir, 'Carrot')
test_parsnip_dir = os.path.join(test_dir, 'Parsnip')

validate_carrot_dir = os.path.join(validate_dir, 'Carrot')
validate_parsnip_dir = os.path.join(validate_dir, 'Parsnip')

"""### Gather meta data about the directory sizes.

"""

num_carrot_train = len(os.listdir(train_carrot_dir))

num_parsnip_train = len(os.listdir(train_parsnip_dir))

num_carrot_test = len(os.listdir(test_carrot_dir))

num_parsnip_test = len(os.listdir(test_parsnip_dir))

num_carrot_val = len(os.listdir(validate_carrot_dir))

num_parsnip_val = len(os.listdir(validate_parsnip_dir))

print('total training carrot images - ', num_carrot_train)
print('total training parsnip images - ', num_parsnip_train)
print('total validation carrot images - ', num_carrot_val)
print('total validation parnips images - ', num_parsnip_val)
print('total testing carrot images - ', num_carrot_test)
print('total testing parsnip images - ', num_parsnip_test)

"""# Data Preparation & Augmentation"""

# Image plotting function
def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
    plt.tight_layout()
    plt.show()

IMG_HEIGHT = 150  
IMG_WIDTH = 150
batch_size = 32

"""1) Horizontal augmentation on training image gen"""

image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)

train_data_gen = image_gen.flow_from_directory(batch_size = batch_size, directory = train_dir,shuffle=True ,target_size=(IMG_HEIGHT,IMG_WIDTH), class_mode='binary')

augmentedImages = [train_data_gen[0][0][0] for i in range(5)]
plotImages(augmentedImages)

"""2) Random Rotation for training set"""

image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45,fill_mode='constant')
train_data_gen = image_gen.flow_from_directory(batch_size=batch_size, directory=train_dir,shuffle=True,target_size=(IMG_HEIGHT,IMG_WIDTH))

augmentedImages = [train_data_gen[0][0][0] for i in range(5)]
plotImages(augmentedImages)

"""3) Random Zoom for training data"""

image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5)
train_data_gen = image_gen.flow_from_directory(batch_size=batch_size, directory=train_dir,shuffle=True,target_size=(IMG_HEIGHT,IMG_WIDTH))

augmentedImages = [train_data_gen[0][0][0] for i in range(5)]
plotImages(augmentedImages)

"""Compiling Augmentations"""

image_gen_train = ImageDataGenerator(rescale=1./255, rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,fill_mode='nearest')
train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,directory=train_dir,shuffle=True,target_size=(IMG_HEIGHT,IMG_WIDTH),class_mode='binary')

augmentedImages = [train_data_gen[0][0][0] for i in range(5)]
plotImages(augmentedImages)

test_data_gen = image_gen.flow_from_directory(directory = test_dir, target_size=(IMG_HEIGHT,IMG_WIDTH), class_mode='binary')

image_gen_non_aug = ImageDataGenerator(rescale=1./255)
validate_data_gen = image_gen_non_aug.flow_from_directory(directory=validate_dir, shuffle=False,target_size=(IMG_HEIGHT,IMG_WIDTH), class_mode='binary')

validationImages = [validate_data_gen[0][0][0] for i in range(5)]
plotImages(validationImages)

sample_training_images, _ = next(train_data_gen)

plotImages(sample_training_images[:5]) #plot images 0-4



"""## Model Definition"""

model = Sequential([tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(IMG_HEIGHT,IMG_WIDTH,3)),
                    tf.keras.layers.MaxPooling2D(2,2),

                    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),
                    tf.keras.layers.MaxPooling2D(2,2),

                    tf.keras.layers.Conv2D(128,(3,3),activation='relu'),
                    tf.keras.layers.MaxPooling2D(2,2),
                    
                    tf.keras.layers.Conv2D(128,(3,3),activation='relu'),
                    tf.keras.layers.MaxPooling2D(2,2),
                    
                    tf.keras.layers.Dropout(0.5),

                    tf.keras.layers.Flatten(),
                    Dense(512,activation='relu'),
                    Dense(2,activation='softmax')
                    ])

"""### Compile Model"""

model.compile(optimizer='adam', loss ='sparse_categorical_crossentropy', metrics=['accuracy'])

"""### Model Summary"""

model.summary()

BATCH_SIZE = 32
EPOCHS = 100

total_train = num_carrot_train + num_parsnip_train
total_val = num_carrot_val + num_parsnip_val
total_test = num_carrot_test + num_parsnip_test

history = model.fit_generator(train_data_gen,
                              steps_per_epoch=int(np.ceil(total_train/float(BATCH_SIZE))),
                              epochs=EPOCHS,
                              validation_data=validate_data_gen,#test_data_gen,
                              validation_steps=int(np.ceil(total_test/float(BATCH_SIZE)))
                              )

"""# Visualizing Results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(EPOCHS)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""## Saving Model

"""

t = time.time()

export_path_keras = "./{}.h5".format(int(t))
print(export_path_keras)

model.save(export_path_keras)

!ls

reloaded = tf.keras.models.load_model(export_path_keras)
reloaded.summary()

image_batch, label_batch = next(iter(train_batches.take(1)))
image_batch = image_batch.numpy()
label_batch = label_batch.numpy()

predicted_batch = model.predict(image_batch)
predicted_batch = tf.squeeze(predicted_batch).numpy()
predicted_ids = np.argmax(predicted_batch, axis=-1)
predicted_class_names = class_names[predicted_ids]
predicted_class_names

results_batch = model.predict(test_dir)
reloaded_result_batch = reloaded.predict(test_dir)

lass_names = np.array(dataset_info.features['label'].names)

print(class_names)
image_batch, label_batch = next(iter(train_batches))


image_batch = image_batch.numpy()
label_batch = label_batch.numpy()

predicted_batch = model.predict(image_batch)
predicted_batch = tf.squeeze(predicted_batch).numpy()

predicted_ids = np.argmax(predicted_batch, axis=-1)
predicted_class_names = class_names[predicted_ids]

print(predicted_class_names)

